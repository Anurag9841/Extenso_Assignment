{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-25T07:27:42.730849Z",
     "start_time": "2024-06-25T07:27:42.715285Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "import pymysql"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:27:43.043346Z",
     "start_time": "2024-06-25T07:27:43.027728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "connection = pymysql.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        password='root',\n",
    "        database='main_database')"
   ],
   "id": "6c0df191b371c794",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:27:43.484266Z",
     "start_time": "2024-06-25T07:27:43.453007Z"
    }
   },
   "cell_type": "code",
   "source": "spark = SparkSession.builder.appName(\"field_mapping\").config(\"spark.jars\", \"C:\\spark-3.5.1-bin-hadoop3\\jars\\mysql-connector-j-8.4.0.jar\").getOrCreate()",
   "id": "8b7d30c1cd28e0f7",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:27:43.816150Z",
     "start_time": "2024-06-25T07:27:43.800557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"jdbc:mysql://localhost:3306/main_database\"\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"root\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}"
   ],
   "id": "45fa8d77e4b68fb",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:27:44.267073Z",
     "start_time": "2024-06-25T07:27:44.251452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sql_table_updater(index,interval_by):\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            try:\n",
    "                exec_date = f'update `main_database`.cf_etl_table set execution_date = current_timestamp where id = {index + 1}'\n",
    "                cursor.execute(exec_date)\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating execution date: {e}\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                start_date = f'update `main_database`.cf_etl_table set start_date_time = date_add(start_date_time, interval {interval_by} day)'\n",
    "                cursor.execute(start_date)\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating start date: {e}\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                end_date = f'update `main_database`.cf_etl_table set end_date_time = date_add(end_date_time, interval {interval_by} day)'\n",
    "                cursor.execute(end_date)\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating end date: {e}\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                connection.commit()\n",
    "            except Exception as e:\n",
    "                print(f\"Error committing transaction: {e}\")\n",
    "                connection.rollback()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in SQL updater: {e}\")"
   ],
   "id": "4e80192c422a52e3",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:27:44.705487Z",
     "start_time": "2024-06-25T07:27:44.674795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mapping(url,table_name,properties):\n",
    "    df = spark.read.jdbc(url=url, table=table_name, properties=properties)\n",
    "    rows = df.collect()\n",
    "    for row in rows:\n",
    "        is_inc = row['is_incremental']\n",
    "        partition_by = row['partition_by']\n",
    "        interval_by = row['interval_days']\n",
    "        id,location,hdfs_file_name,inc_field,database_name,table_name =row['id'],row['location'],row['hdfs_file_name'],row['inc_field'],row['Schema_names'],row['Table_names']\n",
    "        start_date,end_date= row['start_date_time'],row['end_date_time']\n",
    "        hdfs_path = f'{location}{hdfs_file_name}'\n",
    "        cursor = connection.cursor()\n",
    "        cursor.callproc('main_database.executor_mappings',[id,database_name,table_name,is_inc,inc_field,start_date,end_date])\n",
    "        result = cursor.fetchall()\n",
    "        column_names = [desc[0] for desc in cursor.description]\n",
    "\n",
    "        df = spark.createDataFrame(result, schema=column_names)\n",
    "        if is_inc:\n",
    "            df.write.mode('append').parquet(hdfs_path,partitionBy=[partition_by]) \n",
    "            sql_table_updater(rows.index(row),interval_by)\n",
    "        else:\n",
    "            df.write.mode('overwrite').parquet(hdfs_path,partitionBy = [partition_by])  "
   ],
   "id": "842454c6e344341c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:27:57.751104Z",
     "start_time": "2024-06-25T07:27:45.807439Z"
    }
   },
   "cell_type": "code",
   "source": "mapping(url,'cf_etl_table',properties)",
   "id": "919275556ec10d03",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:27:57.876127Z",
     "start_time": "2024-06-25T07:27:57.751104Z"
    }
   },
   "cell_type": "code",
   "source": "data = spark.read.parquet('hdfs://localhost:19000/airflow/sample_data')",
   "id": "24ee76cef9c423a3",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:27:58.063602Z",
     "start_time": "2024-06-25T07:27:57.876127Z"
    }
   },
   "cell_type": "code",
   "source": "data.show()",
   "id": "802f766276259047",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+----------+--------+----------+\n",
      "|Transaction_id|   Transaction_date|Account_id|Products|     Dates|\n",
      "+--------------+-------------------+----------+--------+----------+\n",
      "|             1|2023-06-23 10:30:00|    ACC001|ProductA|2023-06-23|\n",
      "|             2|2023-06-24 11:00:00|    ACC002|ProductB|2023-06-24|\n",
      "|             3|2023-06-25 09:45:00|    ACC003|ProductC|2023-06-25|\n",
      "+--------------+-------------------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1ec96bcec8d19881"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
