{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:34:01.109133Z",
     "start_time": "2024-06-26T04:34:00.811578Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "import pymysql"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:34:08.431276Z",
     "start_time": "2024-06-26T04:34:01.631118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('Field_mapping_modified') \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:19000\") \\\n",
    "    .getOrCreate()"
   ],
   "id": "bac7c902748a81ea",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:34:08.467095Z",
     "start_time": "2024-06-26T04:34:08.433318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "connection = pymysql.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        password='root',\n",
    "        database='main_database')"
   ],
   "id": "e2f870e707abc450",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:34:10.639439Z",
     "start_time": "2024-06-26T04:34:08.467095Z"
    }
   },
   "cell_type": "code",
   "source": "spark = SparkSession.builder.appName(\"field_mapping\").config(\"spark.jars\", \"C:\\spark-3.5.1-bin-hadoop3\\jars\\mysql-connector-j-8.4.0.jar\").getOrCreate()\n",
   "id": "9161a8014b1f0dce",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:34:10.655008Z",
     "start_time": "2024-06-26T04:34:10.639439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"jdbc:mysql://localhost:3306/main_database\"\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"root\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}"
   ],
   "id": "a38686ec234a849f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:34:10.670616Z",
     "start_time": "2024-06-26T04:34:10.655008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sql_table_updater(index,interval_by):\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            try:\n",
    "                exec_date = f'update `main_database`.cf_etl_table set execution_date = current_timestamp where id = {index + 1}'\n",
    "                cursor.execute(exec_date)\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating execution date: {e}\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                start_date = f'update `main_database`.cf_etl_table set start_date_time = date_add(start_date_time, interval {interval_by} day)'\n",
    "                cursor.execute(start_date)\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating start date: {e}\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                end_date = f'update `main_database`.cf_etl_table set end_date_time = date_add(end_date_time, interval {interval_by} day)'\n",
    "                cursor.execute(end_date)\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating end date: {e}\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                connection.commit()\n",
    "            except Exception as e:\n",
    "                print(f\"Error committing transaction: {e}\")\n",
    "                connection.rollback()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in SQL updater: {e}\")"
   ],
   "id": "52592e0ce4c8201c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:35:25.237850Z",
     "start_time": "2024-06-26T04:35:25.222191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mapping(url,table_name,properties):\n",
    "    df = spark.read.jdbc(url=url, table=table_name, properties=properties)\n",
    "    rows = df.collect()\n",
    "    for row in rows:\n",
    "        interval_by,id,location,hdfs_file_name,inc_field,database_name,table_name,start_date,end_date,partition_by,is_inc =row['interval_days'],row['id'],row['location'],row['hdfs_file_name'],row['inc_field'],row['Schema_names'],row['Table_names'],row['start_date_time'],row['end_date_time'],row['partition_by'],row['is_incremental']\n",
    "        hdfs_path = f'{location}{hdfs_file_name}'\n",
    "        cursor = connection.cursor()\n",
    "        cursor.callproc('main_database.executor_mappings_without_cursor', [id])\n",
    "        result = cursor.fetchone()[0]\n",
    "        jdbc_url = f\"jdbc:mysql://localhost:3306/{database_name}\"\n",
    "        if is_inc:\n",
    "            query = f\"(SELECT {result} FROM {database_name}.{table_name} WHERE {inc_field} BETWEEN '{start_date}' AND '{end_date}') as selected_data\"\n",
    "            hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "            fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)\n",
    "            path = spark._jvm.org.apache.hadoop.fs.Path(hdfs_path)\n",
    "            if fs.exists(path):\n",
    "                existing_data = spark.read.parquet(hdfs_path)\n",
    "                dataframe = spark.read.jdbc(url=jdbc_url, table=query, properties=properties)\n",
    "                merged_data = dataframe.unionByName(existing_data).dropDuplicates([partition_by])\n",
    "                merged_data.write.mode('overwrite').parquet(hdfs_path,partitionBy=[partition_by])\n",
    "            else:\n",
    "                dataframe.write.mode('overwrite').parquet(hdfs_path,partitionBy=[partition_by])\n",
    "            sql_table_updater(rows.index(row), interval_by)\n",
    "        else:\n",
    "            query = f\"(SELECT {result} FROM {database_name}.{table_name}) as selected_data\"\n",
    "            dataframe = spark.read.jdbc(url=jdbc_url, table=query, properties=properties)\n",
    "            dataframe.write.mode('overwrite').parquet(hdfs_path) "
   ],
   "id": "a49ce17c704b4572",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:35:27.083763Z",
     "start_time": "2024-06-26T04:35:25.710060Z"
    }
   },
   "cell_type": "code",
   "source": "mapping(url,'cf_etl_table',properties)",
   "id": "340cdbbb80ee0017",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:35:27.259520Z",
     "start_time": "2024-06-26T04:35:27.083763Z"
    }
   },
   "cell_type": "code",
   "source": "data = spark.read.parquet('hdfs://localhost:19000//airflow/sample_data')",
   "id": "369e6625e10e9e14",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:35:28.352054Z",
     "start_time": "2024-06-26T04:35:28.177301Z"
    }
   },
   "cell_type": "code",
   "source": "data.show()",
   "id": "d2b646f94fe1172c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+----------+--------+----------+\n",
      "|Transaction_id|   Transaction_date|Account_id|Products|     Dates|\n",
      "+--------------+-------------------+----------+--------+----------+\n",
      "|             1|2023-06-23 10:30:00|    ACC001|ProductA|2023-06-23|\n",
      "|             2|2023-06-24 11:00:00|    ACC002|ProductB|2023-06-24|\n",
      "|             3|2023-06-25 09:45:00|    ACC003|ProductC|2023-06-25|\n",
      "|             4|2023-06-26 12:15:00|    ACC004|ProductD|2023-06-26|\n",
      "+--------------+-------------------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "665adf7d65a1ecbd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
