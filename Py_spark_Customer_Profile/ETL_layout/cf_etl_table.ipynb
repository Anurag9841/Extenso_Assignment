{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-23T08:05:01.885484Z",
     "start_time": "2024-06-23T08:05:01.160680Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "import pymysql"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T08:05:01.917491Z",
     "start_time": "2024-06-23T08:05:01.888478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "connection = pymysql.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        password='root',\n",
    "        database='main_database'\n",
    "    )"
   ],
   "id": "c294e839a1c3558a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T08:05:11.585692Z",
     "start_time": "2024-06-23T08:05:01.920478Z"
    }
   },
   "cell_type": "code",
   "source": "spark = SparkSession.builder.appName(\"hadoop_add_file\").config(\"spark.jars\", \"C:\\spark-3.5.1-bin-hadoop3\\jars\\mysql-connector-j-8.4.0.jar\").getOrCreate()",
   "id": "fc86977f37d3d8c9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T08:05:11.601098Z",
     "start_time": "2024-06-23T08:05:11.586692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"jdbc:mysql://localhost:3306/main_database\"\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"root\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}"
   ],
   "id": "417d44574bb75f1a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T08:05:20.232935Z",
     "start_time": "2024-06-23T08:05:11.605308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = spark.read.jdbc(url=url, table='cf_etl_table', properties=properties)\n",
    "df.show()\n",
    "is_inc = df.select('is_incremental').collect()[0]['is_incremental']\n",
    "is_inc"
   ],
   "id": "8673bd87108ae501",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------+--------------------+--------------+-------------------+-------------------+--------------+--------------+---------+\n",
      "| id|  Schema_names|Table_names|            location|hdfs_file_name|    start_date_time|      end_date_time|is_incremental|execution_date|inc_field|\n",
      "+---+--------------+-----------+--------------------+--------------+-------------------+-------------------+--------------+--------------+---------+\n",
      "|  1|test_database1|sample_data|hdfs://localhost:...|   sample_data|2023-06-23 00:00:01|2023-06-23 23:59:59|             1|          NULL| Tnx_date|\n",
      "+---+--------------+-----------+--------------------+--------------+-------------------+-------------------+--------------+--------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T08:05:20.514478Z",
     "start_time": "2024-06-23T08:05:20.235938Z"
    }
   },
   "cell_type": "code",
   "source": "df.show()",
   "id": "829dbbd5ed3196d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------+--------------------+--------------+-------------------+-------------------+--------------+--------------+---------+\n",
      "| id|  Schema_names|Table_names|            location|hdfs_file_name|    start_date_time|      end_date_time|is_incremental|execution_date|inc_field|\n",
      "+---+--------------+-----------+--------------------+--------------+-------------------+-------------------+--------------+--------------+---------+\n",
      "|  1|test_database1|sample_data|hdfs://localhost:...|   sample_data|2023-06-23 00:00:01|2023-06-23 23:59:59|             1|          NULL| Tnx_date|\n",
      "+---+--------------+-----------+--------------------+--------------+-------------------+-------------------+--------------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T08:22:53.815563Z",
     "start_time": "2024-06-23T08:22:53.771072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def etl(url,table_name,properties):\n",
    "    df = spark.read.jdbc(url=url, table=table_name, properties=properties)\n",
    "    is_inc = df.select('is_incremental').collect()[0]['is_incremental']\n",
    "    for index in range(df.count()):\n",
    "        if is_inc:\n",
    "            start_date = df.select('start_date_time').collect()[index]['start_date_time']\n",
    "            end_date = df.select('end_date_time').collect()[index]['end_date_time']\n",
    "            location = df.select('location').collect()[index]['location']\n",
    "            hdfs_file_name = df.select('hdfs_file_name').collect()[index]['hdfs_file_name']\n",
    "            inc_field = df.select('inc_field').collect()[index]['inc_field']\n",
    "            database_name = df.select('Schema_names').collect()[index]['Schema_names']\n",
    "            table_name = df.select('Table_names').collect()[index]['Table_names']\n",
    "            hdfs_path = f'{location}{hdfs_file_name}'\n",
    "            jdbc_url = f\"jdbc:mysql://localhost:3306/{database_name}\"\n",
    "            query =  f\"(SELECT * FROM {database_name}.{table_name} WHERE {inc_field} BETWEEN '{start_date}' AND '{end_date}') as selected_data\"\n",
    "            dataframe = spark.read.jdbc(url=jdbc_url, table=query, properties=properties)\n",
    "            dataframe.write.mode('append').parquet(hdfs_path)\n",
    "            dataframe.write.csv('./output_data/out_put.csv',mode = 'append')\n",
    "            with connection.cursor() as cursor:\n",
    "               exec_date = f'update `main_database`.cf_etl_table set execution_date = (current_timestamp) where id = {index +1}' \n",
    "               cursor.execute(exec_date)\n",
    "               start_date = f'update `main_database`.cf_etl_table set start_date_time = date_add(start_date_time, interval 1 day)'\n",
    "               cursor.execute(start_date)\n",
    "               end_date = f'update `main_database`.cf_etl_table set end_date_time = date_add(end_date_time, interval 1 day)'\n",
    "               cursor.execute(end_date)\n",
    "               connection.commit()"
   ],
   "id": "fa8f636578679393",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T08:22:59.409796Z",
     "start_time": "2024-06-23T08:22:57.430376Z"
    }
   },
   "cell_type": "code",
   "source": "etl(url,'cf_etl_table',properties)",
   "id": "f8eaece0333cc967",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1e2df9c94848ee19"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
