{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-24T03:57:27.462563Z",
     "start_time": "2024-06-24T03:57:27.036937Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "import pymysql"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T03:57:28.220242Z",
     "start_time": "2024-06-24T03:57:28.188995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "connection = pymysql.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        password='root',\n",
    "        database='main_database'\n",
    "    )"
   ],
   "id": "c294e839a1c3558a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T03:57:35.105307Z",
     "start_time": "2024-06-24T03:57:28.942729Z"
    }
   },
   "cell_type": "code",
   "source": "spark = SparkSession.builder.appName(\"hadoop_add_file\").config(\"spark.jars\", \"C:\\spark-3.5.1-bin-hadoop3\\jars\\mysql-connector-j-8.4.0.jar\").getOrCreate()",
   "id": "fc86977f37d3d8c9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T03:57:35.862511Z",
     "start_time": "2024-06-24T03:57:35.846892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"jdbc:mysql://localhost:3306/main_database\"\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"root\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}"
   ],
   "id": "417d44574bb75f1a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T03:57:45.101797Z",
     "start_time": "2024-06-24T03:57:37.681481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = spark.read.jdbc(url=url, table='cf_etl_table', properties=properties)\n",
    "df.show()\n",
    "is_inc = df.select('is_incremental').collect()[0]['is_incremental']\n",
    "is_inc"
   ],
   "id": "8673bd87108ae501",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------+--------------------+--------------+-------------------+-------------------+--------------+-------------------+---------+\n",
      "| id|  Schema_names|Table_names|            location|hdfs_file_name|    start_date_time|      end_date_time|is_incremental|     execution_date|inc_field|\n",
      "+---+--------------+-----------+--------------------+--------------+-------------------+-------------------+--------------+-------------------+---------+\n",
      "|  1|test_database1|sample_data|hdfs://localhost:...|   sample_data|2023-06-28 00:00:01|2023-06-28 23:59:59|             1|2024-06-24 09:03:13| Tnx_date|\n",
      "+---+--------------+-----------+--------------------+--------------+-------------------+-------------------+--------------+-------------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T03:57:47.928454Z",
     "start_time": "2024-06-24T03:57:47.772207Z"
    }
   },
   "cell_type": "code",
   "source": "df.show()",
   "id": "829dbbd5ed3196d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------+--------------------+--------------+-------------------+-------------------+--------------+-------------------+---------+\n",
      "| id|  Schema_names|Table_names|            location|hdfs_file_name|    start_date_time|      end_date_time|is_incremental|     execution_date|inc_field|\n",
      "+---+--------------+-----------+--------------------+--------------+-------------------+-------------------+--------------+-------------------+---------+\n",
      "|  1|test_database1|sample_data|hdfs://localhost:...|   sample_data|2023-06-28 00:00:01|2023-06-28 23:59:59|             1|2024-06-24 09:03:13| Tnx_date|\n",
      "+---+--------------+-----------+--------------------+--------------+-------------------+-------------------+--------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T03:57:49.217516Z",
     "start_time": "2024-06-24T03:57:49.186495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sql_updater(index):\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            try:\n",
    "                exec_date = f'update `main_database`.cf_etl_table set execution_date = current_timestamp where id = {index + 1}'\n",
    "                cursor.execute(exec_date)\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating execution date: {e}\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                start_date = f'update `main_database`.cf_etl_table set start_date_time = date_add(start_date_time, interval 1 day)'\n",
    "                cursor.execute(start_date)\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating start date: {e}\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                end_date = f'update `main_database`.cf_etl_table set end_date_time = date_add(end_date_time, interval 1 day)'\n",
    "                cursor.execute(end_date)\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating end date: {e}\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                connection.commit()\n",
    "            except Exception as e:\n",
    "                print(f\"Error committing transaction: {e}\")\n",
    "                connection.rollback()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in SQL updater: {e}\")"
   ],
   "id": "4b34c9c73d2b4b78",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T03:57:53.154671Z",
     "start_time": "2024-06-24T03:57:53.139040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def etl(url, table_name, properties):\n",
    "    try:\n",
    "        df = spark.read.jdbc(url=url, table=table_name, properties=properties)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading from JDBC: {e}\")\n",
    "        return\n",
    "    rows = df.collect()\n",
    "    for row in rows:\n",
    "        is_inc = row['is_incremental']\n",
    "        location,hdfs_file_name,inc_field,database_name,table_name = row['location'],row['hdfs_file_name'],row['inc_field'],row['Schema_names'],row['Table_names']\n",
    "        if is_inc:\n",
    "            start_date,end_date= row['start_date_time'],row['end_date_time']\n",
    "            hdfs_path = f'{location}{hdfs_file_name}'\n",
    "            jdbc_url = f\"jdbc:mysql://localhost:3306/{database_name}\"\n",
    "            query = f\"(SELECT * FROM {database_name}.{table_name} WHERE {inc_field} BETWEEN '{start_date}' AND '{end_date}') as selected_data\"          \n",
    "            dataframe = spark.read.jdbc(url=jdbc_url, table=query, properties=properties)\n",
    "            dataframe.write.mode('append').parquet(hdfs_path)\n",
    "            sql_updater(rows.index(row))\n",
    "        else:\n",
    "            jdbc_url = f\"jdbc:mysql://localhost:3306/{database_name}\"\n",
    "            dataframe = spark.read.jdbc(url=jdbc_url, table=table_name, properties=properties)\n",
    "            hdfs_path = f'{location}{hdfs_file_name}'\n",
    "            dataframe.write.mode(\"overwrite\").parquet(hdfs_path)  "
   ],
   "id": "fa8f636578679393",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T03:57:58.913718Z",
     "start_time": "2024-06-24T03:57:55.968125Z"
    }
   },
   "cell_type": "code",
   "source": "etl(url,'cf_etl_table',properties)",
   "id": "f8eaece0333cc967",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T03:58:01.907844Z",
     "start_time": "2024-06-24T03:58:01.398489Z"
    }
   },
   "cell_type": "code",
   "source": "data = spark.read.parquet('hdfs://localhost:19000/anurag/sample_data')",
   "id": "1e2df9c94848ee19",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T03:59:20.463954Z",
     "start_time": "2024-06-24T03:59:20.401418Z"
    }
   },
   "cell_type": "code",
   "source": "df = spark.read.jdbc(url=url, table='cf_etl_table', properties=properties)",
   "id": "ee68d126696469ff",
   "outputs": [],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
